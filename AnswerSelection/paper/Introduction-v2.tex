\section{Introduction}
%What is the problem and why is it important? Why is it hard?

%A fundamental challenge in ranking user-generated content in social networks is: how to identify relevant content, in response to a query (a search problem); or in response to user behavior (a recommendation problem).
Individuals often visit Community Question Answer (CQA) forums, like StackExchange, to seek answers to nuanced questions that are not readily answered via web-search engines. Unlike other familiar Learning-to-Rank problems in the IR community~\cite{LambdaMart,LambdaNet}, CQA platforms can identify and leverage past questions asked by similar users and relevant answers to those questions. However, there is only limited work in the context of identification of ``best answers'' among user-generated content that exploit these implicit and explicit connections.

A well-studied approach is to identify salient features for each question-answer tuple $(q,a)$ in an inductive supervised classification setting~\cite{BurelMA16,JendersKN16,TianZL13}. In this vein, neural text models exploit textual features~\cite{ZhangLSW17,WuWS18,WangN15} by learning effective representations of $(q,a)$ tuples. While the neural feature models are effective, there are limitations to examining $(q,a)$ tuples in isolation: an answer is adjudged "best" \emph{in relationship} to other answers to the same question. Further, considering other answers to similar questions, as well as those from similar users, can provide cues to answer quality.
%and other answers to similar questions, as well as those from similar users. Further, users answer multiple questions providing hints to their expertise.
These relational aspects of user-generated content provide a unique dimension that is absent in textual search. Our key proposal is to build a flexible and expressive framework to incorporate the relational aspects of user-generated content for the answer selection task. %content-labeling mechanism.

Relational aspects are best captured as graphs connecting content. Graph Convolutional Networks (GCNs) are shown to be an effective approach to incorporate attributed graph structure in tasks such as node classification \cite{gcn} and link prediction~\cite{relationalGCN}. While GCNs are a plausible approach, we need to overcome a fundamental implicit assumption in prior work before we can apply it to our problem. Prior work in GCNs adopt label sharing amongst nodes; label sharing implicitly assumes similarity between two nodes connected by an edge. Extensions to the basic GCN model such as signed networks~\cite{signedgcn} and multi-relation networks~\cite{DualGCN,relationalGCN} do not address the fundamental challenge of modeling label contrast. For instance, in the answer selection problem, if we link together answers to the same question, they do not share acceptance labels. We label an answer as `accepted' by contrast to other answers to the same question. In other words, the relational views (or graphs) could capture similarity or contrast between connected content, depending on the relation in consideration.

We develop a novel framework to model the diverse relations between content through a separate \textit{induced} graph across $(q,a)$ tuples. The key idea is to then use distinct label selection mechanisms depending on the semantics of the relational view. For instance, in the trivial case, the label depends only on the answer features in the \textit{reflexive} view (i.e., no edges), or the label contrasts to other connected answers in a \textit{contrastive} view, or the label is shared among similar answers to different questions in a \textit{similarity} view. This generalizes to a broader principle: pick equivalence relations to induce graphs comprising cliques, and then pick an appropriate label selection mechanism (label sharing or label contrast) for the graph. We show how to develop convolutional architectures to achieve the sharing and contrast label selection mechanisms. Then, we aggregate results across different relational views through a boosting framework to identify the label for each $(q,a)$ tuple. In summary, our contributions are as follows:

%We develop a novel induced relational framework to address the diversity of relational views across content. The key idea is to use diverse strategies---label depends only on the answer features (reflexive), label is determined in contrast with the other connected answers (contrastive), and label sharing among answers across questions if it contrasts with other answers similarly(similarity by contrast)---to identify the accepted answer. Each strategy \textit{induces} a graph between $(q,a)$ tuples and then uses a particular label selection mechanism to identify the accepted answer. Our strategies generalize to a broader principle: pick an equivalence relation to induce a graph comprising cliques, and then pick a label selection mechanism (label sharing or label contrast) within each clique. We show how to develop GCN architecture to operationalize the specific label selection mechanism (label sharing or label contrast). Then, we aggregate results across strategies through a boosting framework to identify the label for each $(q,a)$ tuple. In summary, our contributions are as follows:
%
%Signed GCNs~\cite{signedgcn} can not capture this contrast despite their ability to incorporate signed edges. Graph attention networks ~\cite{graphattention} also could not learn negative attention weight over neighbors as weights are the output of a softmax operation.

%The problem is hard for two reasons: first, the "relevant" answer is determined \emph{in relationship} to other pieces of answer in the network; second, the interaction structure among the participants of the social network influences identification of appropriate content. The problem is especially crucial to Community Question Answer platforms such as Stack-Exchanges, where individuals visit to seek answers to nuanced questions, not readily available on prominent search engines.

% While there are clear connections to well known Learning to Rank problems in the IR community ~ \cite{LambdaNet, LambdaMart, LearningtoRank}, there is only limited work in the context of identification of "best answers" among user-generated answers on CQA forums using relational information. Induced relational views (or graphs) capture the inter-connected structure of user-generated answers with potentially varying view semantics, for instance, a similarity view would seek to cluster similar content. On the other hand, a contrastive view would attempt to connect an answer against other competing submissions. Note that induced contrastive view is distinct from signed graphs where edges are either positive or negative. Signed graph convolution~\cite{signedgcn} gathers the friends and enemies of a given node and computes separate embeddings through feature sharing. This separate convolution does not achieve node feature contrast. Also, signed edges are harvested from curated data, unlike our induced links. Graph attention networks ~\cite{graphattention} also could not learn negative attention weight over neighbors because weights are the output of a softmax operation.


% Recent work in merging distinct multi-relations between nodes includes work on using Neural techniques on graphs, such as DualGCN~\cite{DualGCN} to capture multiple modalities of the data. The main limitation of the work in Convolutional Networks on graphs is the focus on label sharing among network nodes; a concept that is natural to many problem contexts (e.g., computer vision problems), but problematic in the case of identification of best content that also requires label contrasts across edges of a graph.



% Identifying credible information in Community QA (CQA) platforms is essential in this age of misinformation and fake news. CQA platforms like Reddit or StackExchange are not just used to answer factual questions but are increasingly also used for open-ended, advice-seeking, and reasoning questions. In these scenarios, it becomes non-trivial to identify credible answers.

%who else has worked on the problem? What did they find? This a summary of the critical findings of related work
% Most of the literature on Answer selection posits it as a classification or learning to rank (LtoR) problem. That is, the prediction is made for each question-answer pair(or question-all candidate answers in case of LtoR) in isolation. They rarely share information of answering user's behavior across the platform.

%what did you do? Here, I explain the algorithm/framework in a precise manner. What exactly is it? Why was it essential to develop it the way you did?


\begin{description}
  \item[\textbf{Modular, Induced Relational Framework:}] We introduce a modular framework that separates the construction of graphs from the label selection mechanism. While prior work in answer selection considers content in isolation~\cite{BurelMA16,JendersKN16}, we use equivalence relations to induce graphs comprising cliques and apply label contrast or label sharing to each graph in our Induced Relational GCN (IR-GCN) framework. Our framework applies to other application semantics involving graphs \cite{InducedGraph}.
  %social or item
  \item[\textbf{Discriminative Semantics:}] We develop a label contrast GCN to differentiate connected vertices in a contrastive view. While prior work in graph convolution (e.g.,~\cite{gcn,DualGCN}) emphasizes node similarity or edge similarity~\cite{signedgcn}, we show that our contrast encoding creates \textit{discriminative magnification}. We enhance the separation of contrasting nodes in the embedding space.
% \item[Induced Relational Framework:] We introduced a novel idea of using strategies---Contrastive, Similarity by contrast, Reflexive---to induce semantically different graphs on question-answer tuples in a CQA forum. In contrast, related work typically operates on knowledge graphs with edges of a specific relation type (e.g., similarity). The contrastive strategy highlights the differences between content. We operationalize similarity amongst answers created by the same individual; we say that two answers are similar when the same individual who created them is different in a precise sense from other peers who have also contributed competing answers. The reflexive strategy is the case when the answer is judged on its merit, disregarding competing answers. The impact of this framework lies in its ability to induce strategy-dependent graphs that can be quite different from the underlying social interaction graph.
%   \item[Operationalizing Relational GCN:] We show how to operationalize each strategy type to a Graph Convolutional Network architecture. The related work has primarily focused on architectures that support label sharing among network neighbors. We show that the contrastive GCN and the similarity by contrast GCN are necessary for addressing our problem.
  \item[\textbf{Boosted Architecture:}]  We show through extensive empirical results that boosting techniques applied across relational views improves learning in our convolutional model. Much of the past work on neural architectures develop stacking, fusion, or aggregator architectures to incorporate multiple views. In contrast, boosting proves a simple and effective strategy in the multi-view setting.
  %We show empirically that a boosted 4-layer GCN works better than a 12-layer stacked architecture. These aggregator architectures perform worse than the 4-layer Contrastive relational GCN for StackExchange. We conjecture that induced graphs contain noise since the links amongst nodes are dependent on the data values; stacking results in noise amplification.
\end{description}

We conducted extensive experiments using our IR-GCN framework with excellent experimental results on the popular CQA forum---StackExchange. For our analysis, we collect data from 50 communities---the ten largest communities from each of the five StackExchange\footnote{https://stackexchange.com/sites} categories. We achieved an improvement of over 4\% in accuracy and 2.5\% in MRR, on average, over state-of-the-art baselines. We also show that our model is more robust to label sparsity compared to multi-relational GCNs. %On StackExchange, individuals who post questions may label an answer as `accepted,' but other questions with answers (about $47$\% in our analysis) have none labeled as `accepted.' 
\footnote{We provide Reddit (\url{https://www.reddit.com/}) results in our supplementary material using expert answers as a proxy for acceptance, to overcome the absence of explicit labels.}

We organize the rest of this paper as follows. In \cref{sec:problem}, we formulate our problem statement and then discuss induced relational views for the answer selection problem in \cref{sec:Induced Relational Views}. We then detail the modeling of these views in our convolution framework in \cref{sec:gcn} and introduce our gradient boosting aggregation approach in \cref{sec:aggregation}. In \cref{sec:experiments}, we describe our experiments, related work in \cref{sec:related} and then conclude in \cref{sec:conclusion}.
